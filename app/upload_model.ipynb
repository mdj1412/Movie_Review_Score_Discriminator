{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "sst2_dataset = load_dataset('sst2')\n",
    "\n",
    "save_df = pd.DataFrame(columns=['eng', 'kor'], index=range(0, 50))\n",
    "kor_df = pd.read_csv('../data/' + \"test_save_2023_01_20_01-13-20.tsv\", sep='\\t', index_col='Unnamed: 0')\n",
    "\n",
    "idx = 0\n",
    "for i in range(len(sst2_dataset['train'])):\n",
    "    if idx == 50: break\n",
    "    text = sst2_dataset['train'][i]['sentence']\n",
    "    if len(text) < 20:\n",
    "        save_df.loc[idx, 'eng'] = text\n",
    "        idx+=1\n",
    "\n",
    "filt = kor_df['text_length'] < 20\n",
    "save_df['kor'][0:50] = kor_df.loc[filt, 'text'][0:50]\n",
    "\n",
    "\n",
    "save_df.to_csv('examples.csv', index=True, sep='\\t')\n",
    "save_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Kor', '별로에요 생각보다 노잼임'],\n",
       "  ['Kor', '진짜 강추 최고의 한국영화'],\n",
       "  ['Kor', '진짜최악입니다...명절에보세요'],\n",
       "  ['Kor', '이시대 최고의 코미디 영화'],\n",
       "  ['Kor', '노우잼스ㅡ  이만잡 열자 채우기']],\n",
       " [['Eng', 'heroes '],\n",
       "  ['Eng', ', plodding picture '],\n",
       "  ['Eng', 'inane and awful '],\n",
       "  ['Eng', 'collapse '],\n",
       "  ['Eng', 'to spare wildlife ']])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "examples = []\n",
    "df = pd.read_csv('examples.csv', sep='\\t', index_col='Unnamed: 0')\n",
    "np.random.seed(100)\n",
    "\n",
    "idx = np.random.choice(50, size=5, replace=False)\n",
    "for i in idx: examples.append(['Eng', df.iloc[i, 0]])\n",
    "kor_examples = [ ['Kor', df.iloc[i, 1]] for i in idx ]\n",
    "\n",
    "kor_examples, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "class LanguageIdentification:\n",
    "\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = \"./lid.176.ftz\"\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def predict_lang(self, text):\n",
    "        predictions = self.model.predict(text, k=200) # returns top 2 matching languages\n",
    "        return predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    LANGUAGE = LanguageIdentification()\n",
    "    # lang = LANGUAGE.predict_lang(\"한글\")\n",
    "    lang = LANGUAGE.predict_lang(\"no lika da\")\n",
    "    # print( lang )\n",
    "    tmp = list(lang[0])\n",
    "    print( lang[0].index('__label__en') )\n",
    "    print( tmp.index('__label__en') )\n",
    "    print( tmp.index('__label__ko') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "/Users/mindongjun/Desktop/무제 폴더/classification/venv/lib/python3.8/site-packages/gradio/inputs.py:217: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "/Users/mindongjun/Desktop/무제 폴더/classification/venv/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7903\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7903/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import fasttext\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "\n",
    "title = \"Movie Review Score Discriminator\"\n",
    "description = \"It is a program that classifies whether it is positive or negative by entering movie reviews.  \\\n",
    "                You can choose between the Korean version and the English version.  \\\n",
    "                It also provides a version called \"\"Default\"\", which determines whether it is Korean or English and predicts it.\"\n",
    "\n",
    "\n",
    "class LanguageIdentification:\n",
    "    def __init__(self):\n",
    "        pretrained_lang_model = \"./lid.176.ftz\"\n",
    "        self.model = fasttext.load_model(pretrained_lang_model)\n",
    "\n",
    "    def predict_lang(self, text):\n",
    "        predictions = self.model.predict(text, k=200) # returns top 200 matching languages\n",
    "        return predictions\n",
    "\n",
    "LANGUAGE = LanguageIdentification()\n",
    "\n",
    "\n",
    "\n",
    "def tokenized_data(tokenizer, inputs):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        [inputs],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=64,\n",
    "        truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "examples = []\n",
    "df = pd.read_csv('examples.csv', sep='\\t', index_col='Unnamed: 0')\n",
    "np.random.seed(100)\n",
    "\n",
    "idx = np.random.choice(50, size=5, replace=False)\n",
    "eng_examples = [ ['Eng', df.iloc[i, 0]] for i in idx ]\n",
    "kor_examples = [ ['Kor', df.iloc[i, 1]] for i in idx ]\n",
    "examples = eng_examples + kor_examples\n",
    "\n",
    "\n",
    "\n",
    "eng_model_name = \"roberta-base\"\n",
    "eng_step = 1900\n",
    "eng_tokenizer = AutoTokenizer.from_pretrained(eng_model_name)\n",
    "eng_file_name = \"{}-{}.pt\".format(eng_model_name, eng_step)\n",
    "eng_state_dict = torch.load(eng_file_name)\n",
    "eng_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    eng_model_name, num_labels=2, id2label=id2label, label2id=label2id,\n",
    "    state_dict=eng_state_dict\n",
    ")\n",
    "\n",
    "\n",
    "kor_model_name = \"klue/roberta-small\"\n",
    "kor_step = 2400\n",
    "kor_tokenizer = AutoTokenizer.from_pretrained(kor_model_name)\n",
    "kor_file_name = \"{}-{}.pt\".format(kor_model_name.replace('/', '_'), kor_step)\n",
    "kor_state_dict = torch.load(kor_file_name)\n",
    "kor_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    kor_model_name, num_labels=2, id2label=id2label, label2id=label2id,\n",
    "    state_dict=kor_state_dict\n",
    ")\n",
    "\n",
    "\n",
    "def builder(Lang, Text):\n",
    "    percent_kor, percent_eng = 0, 0\n",
    "    text_list = Text.split(' ')\n",
    "\n",
    "\n",
    "    # [ output_1 ]\n",
    "    if Lang == '언어감지 기능 사용':\n",
    "        pred = LANGUAGE.predict_lang(Text)\n",
    "        if '__label__en' in pred[0]:\n",
    "            Lang = 'Eng'\n",
    "            idx = pred[0].index('__label__en')\n",
    "            p_eng = pred[1][idx]\n",
    "        if '__label__ko' in pred[0]:\n",
    "            Lang = 'Kor'\n",
    "            idx = pred[0].index('__label__ko')\n",
    "            p_kor = pred[1][idx]\n",
    "        # Normalize Percentage\n",
    "        percent_kor = p_kor / (p_kor+p_eng)\n",
    "        percent_eng = p_eng / (p_kor+p_eng)\n",
    "\n",
    "    if Lang == 'Eng':\n",
    "        model = eng_model\n",
    "        tokenizer = eng_tokenizer\n",
    "        if percent_eng==0: percent_eng=1\n",
    "\n",
    "    if Lang == 'Kor':\n",
    "        model = kor_model\n",
    "        tokenizer = kor_tokenizer\n",
    "        if percent_kor==0: percent_kor=1\n",
    "        \n",
    "\n",
    "    # [ output_2 ]\n",
    "    inputs = tokenized_data(tokenizer, Text)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids=inputs['input_ids'], \n",
    "            attention_mask=inputs['attention_mask']).logits\n",
    "    \n",
    "    m = torch.nn.Softmax(dim=1)\n",
    "    output = m(logits)\n",
    "    # print(logits, output)\n",
    "\n",
    "\n",
    "    # [ output_3 ]\n",
    "    output_analysis = []\n",
    "    for word in text_list:\n",
    "        tokenized_word = tokenized_data(tokenizer, word)\n",
    "        with torch.no_grad():\n",
    "            logit = model(input_ids=tokenized_word['input_ids'], \n",
    "                attention_mask=tokenized_word['attention_mask']).logits\n",
    "        word_output = m(logit)\n",
    "        if word_output[0][1] > 0.99:\n",
    "            output_analysis.append( (word, '+++') )\n",
    "        elif word_output[0][1] > 0.9:\n",
    "            output_analysis.append( (word, '++') )\n",
    "        elif word_output[0][1] > 0.8:\n",
    "            output_analysis.append( (word, '+') )\n",
    "        elif word_output[0][1] < 0.01:\n",
    "            output_analysis.append( (word, '---') )\n",
    "        elif word_output[0][1] < 0.1:\n",
    "            output_analysis.append( (word, '--') )\n",
    "        elif word_output[0][1] < 0.2:\n",
    "            output_analysis.append( (word, '-') )\n",
    "        else:\n",
    "            output_analysis.append( (word, None) )\n",
    "    \n",
    "\n",
    "    return [ {'Kor': percent_kor, 'Eng': percent_eng}, \n",
    "            {id2label[1]: output[0][1].item(), id2label[0]: output[0][0].item()}, \n",
    "            output_analysis ]\n",
    "            \n",
    "    # prediction = torch.argmax(logits, axis=1)\n",
    "    return id2label[prediction.item()]\n",
    "\n",
    "\n",
    "# demo3 = gr.Interface.load(\"models/mdj1412/movie_review_score_discriminator_eng\", inputs=\"text\", outputs=\"text\", \n",
    "#                          title=title, theme=\"peach\",\n",
    "#                          allow_flagging=\"auto\",\n",
    "#                          description=description, examples=examples)\n",
    "\n",
    "\n",
    "\n",
    "# demo = gr.Interface(builder, inputs=[gr.inputs.Dropdown(['Default', 'Eng', 'Kor']), gr.Textbox(placeholder=\"리뷰를 입력하시오.\")], \n",
    "#                     outputs=[ gr.Label(num_top_classes=3, label='Lang'), \n",
    "#                             gr.Label(num_top_classes=2, label='Result'),\n",
    "#                             gr.HighlightedText(label=\"Analysis\", combine_adjacent=False)\n",
    "#                             .style(color_map={\"+++\": \"#CF0000\", \"++\": \"#FF3232\", \"+\": \"#FFD4D4\", \"---\": \"#0004FE\", \"--\": \"#4C47FF\", \"-\": \"#BEBDFF\"}) ],\n",
    "#                     # outputs='label',\n",
    "#                     title=title, description=description, examples=examples)\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo1:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    <h1 align=\"center\">\n",
    "    Movie Review Score Discriminator\n",
    "    </h1>\n",
    "    \"\"\")\n",
    "\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    영화 리뷰를 입력하면, 리뷰가 긍정인지 부정인지 판별해주는 모델이다. \\\n",
    "    영어와 한글을 지원하며, 언어를 직접 선택할수도, 혹은 모델이 언어감지를 직접 하도록 할 수 있다.  \n",
    "    리뷰를 입력하면, (1) 감지된 언어, (2) 긍정 리뷰일 확률과 부정 리뷰일 확률, (3) 입력된 리뷰의 어느 단어가 긍정/부정 결정에 영향을 주었는지 \\\n",
    "    (긍정일 경우 빨강색, 부정일 경우 파란색)를 확인할 수 있다.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Accordion(label=\"모델에 대한 설명 ( 여기를 클릭 하시오. )\", open=False):\n",
    "        gr.Markdown(\n",
    "        \"\"\"\n",
    "        영어 모델은 bert-base-uncased 기반으로, 영어 영화 리뷰 분석 데이터셋인 SST-2로 학습 및 평가되었다.  \n",
    "        한글 모델은 klue/roberta-base 기반이다. 기존 한글 영화 리뷰 분석 데이터셋이 존재하지 않아, 네이버 영화의 리뷰를 크롤링해서 영화 리뷰 분석 데이터셋을 제작하고, 이를 이용하여 모델을 학습 및 평가하였다.  \n",
    "        영어 모델은 SST-2에서 92.8%, 한글 모델은 네이버 영화 리뷰 데이터셋에서 94%의 정확도를 가진다 (test set 기준).  \n",
    "        언어감지는 fasttext의 language detector를 사용하였다. 리뷰의 단어별 영향력은, 단어 각각을 모델에 넣었을 때 결과가 긍정으로 나오는지 부정으로 나오는지를 바탕으로 측정하였다.\n",
    "        \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            inputs_1 = gr.Dropdown(choices=['언어감지 기능 사용', 'Eng', 'Kor'], value='언어감지 기능 사용', label='Lang')\n",
    "            inputs_2 = gr.Textbox(placeholder=\"리뷰를 입력하시오.\", label='Text')\n",
    "            with gr.Row():\n",
    "                # btn2 = gr.Button(\"클리어\")\n",
    "                btn = gr.Button(\"제출하기\")\n",
    "        with gr.Column():\n",
    "            output_1 = gr.Label(num_top_classes=3, label='Lang')\n",
    "            output_2 = gr.Label(num_top_classes=2, label='Result')\n",
    "            output_3 = gr.HighlightedText(label=\"Analysis\", combine_adjacent=False) \\\n",
    "                .style(color_map={\"+++\": \"#CF0000\", \"++\": \"#FF3232\", \"+\": \"#FFD4D4\", \"---\": \"#0004FE\", \"--\": \"#4C47FF\", \"-\": \"#BEBDFF\"})\n",
    "    \n",
    "    # btn2.click(fn=fn2, inputs=[None, None], output=[output_1, output_2, output_3])\n",
    "    btn.click(fn=builder, inputs=[inputs_1, inputs_2], outputs=[output_1, output_2, output_3])\n",
    "    gr.Examples(examples, inputs=[inputs_1, inputs_2])\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # print(examples)\n",
    "    # demo.launch()\n",
    "    demo1.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b759cee5f05774718d2a64abc3f0ceef5319288676700299b101fa9a235f3f57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
